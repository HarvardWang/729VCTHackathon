import requests
from bs4 import BeautifulSoup
import pandas as pd
import os

def makeFileName(url):
    path = url.replace('https://liquipedia.net/valorant/', '')
    filename = path.replace('/', '_')
    return f'{filename}.csv'

def names(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    table = soup.find('table')
    
    if table is None:
        return None
    
    player_names = []
    
    for row in table.find_all('tr')[1:]:
        cells = row.find_all(['th', 'td'])
        for cell in cells:

            if cell.find('a'):
                player_names.append(cell.text.strip())
                break
    df = pd.DataFrame(player_names, columns=['Player Name'])
    
    return df

# Change this part for each link
url = 'https://liquipedia.net/valorant/VCL/2024/Oceania/Split_2/Statistics'

data = names(url)

if data is not None:
    filename = makeFileName(url)
    filepath = os.path.join('.', filename)
    data.to_csv(filepath, index=False)
